{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e3ed81",
   "metadata": {},
   "source": [
    "- Combined train and valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04436e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "from urlextract import URLExtract\n",
    "import contractions\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6970786",
   "metadata": {},
   "source": [
    "- Merging Train and Valid Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2556546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Here are Thursday's biggest analyst calls: App...      0\n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
      "3  Analysts react to Tesla's latest earnings, bre...      0\n",
      "4  Netflix and its peers are set for a ‘return to...      0\n",
      "21107\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and valid dataset\n",
    "train_df = pd.read_csv(\"Dataset\\\\topic_train.csv\")\n",
    "valid_df = pd.read_csv(\"Dataset\\\\topic_valid.csv\")\n",
    "\n",
    "# Merge both dataset\n",
    "combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset \n",
    "combined_df.to_csv(\"Dataset\\\\merged_datasetA.csv\", index=False)\n",
    "\n",
    "# Display first few rows \n",
    "print(combined_df.head())\n",
    "print(len(combined_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd921c",
   "metadata": {},
   "source": [
    "- Preprocess FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be0dc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Here are Thursday's biggest analyst calls: App...      0\n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
      "3  Analysts react to Tesla's latest earnings, bre...      0\n",
      "4  Netflix and its peers are set for a ‘return to...      0\n",
      "21107\n"
     ]
    }
   ],
   "source": [
    "#Load combined dataset\n",
    "finpep_df = pd.read_csv(\"Dataset\\\\merged_datasetA.csv\")\n",
    "print(finpep_df.head())\n",
    "print( len(finpep_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c18275da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21107/21107 [04:41<00:00, 74.94it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>Here are thursdays biggest analyst calls Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>Buy as vegas hands as travel to singapore buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>piper handler downgrades DocuSign to sell citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>analysts react to Teslas latest earnings break...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>netflix and its peers are set for a return to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Here are Thursday's biggest analyst calls: App...   \n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...   \n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...   \n",
       "3  Analysts react to Tesla's latest earnings, bre...   \n",
       "4  Netflix and its peers are set for a ‘return to...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  Here are thursdays biggest analyst calls Apple...  \n",
       "1  Buy as vegas hands as travel to singapore buil...  \n",
       "2  piper handler downgrades DocuSign to sell citi...  \n",
       "3  analysts react to Teslas latest earnings break...  \n",
       "4  netflix and its peers are set for a return to ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ticker & acronym whitelist\n",
    "def detect_protected_terms(text_series):\n",
    "    \"\"\"Detect uppercase tickers/acronyms from the dataset.\"\"\"\n",
    "    pattern = r'\\$[A-Z]{1,5}\\b|\\b[A-Z]{2,5}\\b'\n",
    "    terms = set()\n",
    "    for text in text_series.dropna():\n",
    "        matches = re.findall(pattern, text)\n",
    "        matches = [m.lstrip('$') for m in matches]  # Remove $ for consistency\n",
    "        terms.update(matches)\n",
    "    return terms\n",
    "\n",
    "protected_words = set([\n",
    "    \"USD\", \"EUR\", \"AAPL\", \"TSLA\", \"GOOG\", \"MSFT\", \"SEC\", \"GDP\", \"NASDAQ\", \"NYSE\"\n",
    "])\n",
    "\n",
    "# SymSpell setup\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1, prefix_length=7)\n",
    "dictionary_path = \"SymSpell_Dictionary/frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def spell_correct(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in protected_words:  # Protect finance terms\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=1)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    def is_valid_text(txt):\n",
    "        return isinstance(txt, str) and txt.strip() != ''\n",
    "\n",
    "    def is_english(txt, target_language='en'):\n",
    "        try:\n",
    "            return detect(txt) == target_language\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def text_cleaning(txt):\n",
    "        # 1. Expand contractions (e.g., don't → do not)\n",
    "        txt = contractions.fix(txt)\n",
    "\n",
    "        # 2. Replace emojis with descriptive text\n",
    "        txt = emoji.replace_emoji(txt, replace=lambda e, _: emoji.demojize(e).strip(':').replace('_', ' '))\n",
    "\n",
    "        # 3. Remove HTML tags\n",
    "        txt = BeautifulSoup(txt, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "        # 4. Remove URLs\n",
    "        extractor = URLExtract()\n",
    "        urls = extractor.find_urls(txt)\n",
    "        for url in urls:\n",
    "            txt = txt.replace(url, '')\n",
    "\n",
    "        # 5. Remove unwanted punctuation but KEEP $, %, +, -\n",
    "        punctuation_to_remove = ''.join(ch for ch in string.punctuation if ch not in ['$','%','+','-'])\n",
    "        txt = txt.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "\n",
    "        # 6. Replace multiple spaces with single space\n",
    "        txt = re.sub(r'\\s+', ' ', txt).strip()\n",
    "\n",
    "        # 7. Optional: Spell correction (safe for finance terms)\n",
    "        txt = spell_correct(txt)\n",
    "\n",
    "        return txt\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    if not is_valid_text(text):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    if not is_english(text):\n",
    "        return \"\"\n",
    "\n",
    "    return text_cleaning(text)\n",
    "\n",
    "\n",
    "# Usage with progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "protected_words = detect_protected_terms(finpep_df['text'])\n",
    "\n",
    "finpep_df['preprocessed_text'] = finpep_df['text'].progress_apply(preprocess)\n",
    "\n",
    "# Remove empty rows\n",
    "finpep_df = finpep_df[finpep_df['preprocessed_text'].str.strip() != '']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "finpep_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "finpep_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa88b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Metric         Before         After\n",
      "0   Avg text length (chars)     138.265262    107.217952\n",
      "1   Avg text length (words)      18.639970     17.550558\n",
      "2                Total URLs   23290.000000      0.000000\n",
      "3              Total Emojis     662.000000      0.000000\n",
      "4  Total Special Characters  195854.000000  24333.000000\n",
      "5    Total Corrections Made       0.000000      0.000000\n"
     ]
    }
   ],
   "source": [
    "# Statistic Analyze After Cleaning\n",
    "correction_counter = Counter()\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'http\\S+|www\\.\\S+', text))\n",
    "\n",
    "def count_emojis(text):\n",
    "    return len(emoji.emoji_list(text))\n",
    "\n",
    "def count_special_chars(text):\n",
    "    return len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "\n",
    "# Create metrics before and after\n",
    "insights = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Avg text length (chars)\",\n",
    "        \"Avg text length (words)\",\n",
    "        \"Total URLs\",\n",
    "        \"Total Emojis\",\n",
    "        \"Total Special Characters\",\n",
    "        \"Total Corrections Made\"\n",
    "    ],\n",
    "    \"Before\": [\n",
    "        finpep_df['text'].str.len().mean(),\n",
    "        finpep_df['text'].str.split().apply(len).mean(),\n",
    "        finpep_df['text'].apply(count_urls).sum(),\n",
    "        finpep_df['text'].apply(count_emojis).sum(),\n",
    "        finpep_df['text'].apply(count_special_chars).sum(),\n",
    "        sum(correction_counter.values())\n",
    "    ],\n",
    "    \"After\": [\n",
    "        finpep_df['preprocessed_text'].str.len().mean(),\n",
    "        finpep_df['preprocessed_text'].str.split().apply(len).mean(),\n",
    "        finpep_df['preprocessed_text'].apply(count_urls).sum(),\n",
    "        finpep_df['preprocessed_text'].apply(count_emojis).sum(),\n",
    "        finpep_df['preprocessed_text'].apply(count_special_chars).sum(),\n",
    "        sum(correction_counter.values()) \n",
    "    ]\n",
    "})\n",
    "\n",
    "print(insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0392c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Here are Thursday's biggest analyst calls: App...   \n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...   \n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...   \n",
      "3  Analysts react to Tesla's latest earnings, bre...   \n",
      "4  Netflix and its peers are set for a ‘return to...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  Here are thursdays biggest analyst calls Apple...  \n",
      "1  Buy as vegas hands as travel to singapore buil...  \n",
      "2  piper handler downgrades DocuSign to sell citi...  \n",
      "3  analysts react to Teslas latest earnings break...  \n",
      "4  netflix and its peers are set for a return to ...  \n",
      "20165\n"
     ]
    }
   ],
   "source": [
    "finpep_df.to_csv(\"Dataset/merged_datasetA(cleaned).csv\", columns=['preprocessed_text','text'],index=False)\n",
    "print(finpep_df.head())\n",
    "print( len(finpep_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e5dfc",
   "metadata": {},
   "source": [
    "Label merged_datasetA(cleaned) with FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0399ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "  0%|          | 0/20163 [00:00<?, ?it/s]c:\\Python_Env\\fyp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 20163/20163 [13:47<00:00, 24.38it/s]\n",
      "  0%|          | 0/20163 [00:00<?, ?it/s]c:\\Python_Env\\fyp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 20163/20163 [14:03<00:00, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  Here are thursdays biggest analyst calls Apple...   \n",
      "1  Buy as vegas hands as travel to singapore buil...   \n",
      "2  piper handler downgrades DocuSign to sell citi...   \n",
      "3  analysts react to Teslas latest earnings break...   \n",
      "4  netflix and its peers are set for a return to ...   \n",
      "\n",
      "                                                text sentiment     score  \n",
      "0  Here are Thursday's biggest analyst calls: App...   Neutral  0.999992  \n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...   Neutral  0.999866  \n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...  Negative  0.999097  \n",
      "3  Analysts react to Tesla's latest earnings, bre...   Neutral  0.999918  \n",
      "4  Netflix and its peers are set for a ‘return to...  Positive  0.999999  \n",
      "20163\n",
      "Exported to twitter_labeled.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # pretrained FinBERT for financial sentiment\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "finclas_df= pd.read_csv(\"Dataset/merged_datasetA(cleaned).csv\")  # Make sure it has a column like 'text'\n",
    "\n",
    "# Add a new column for sentiment label\n",
    "tqdm.pandas()\n",
    "finclas_df['sentiment'] = finclas_df['preprocessed_text'].progress_apply(lambda x: sentiment_pipeline(x)[0]['label'])\n",
    "\n",
    "finclas_df['score'] = finclas_df['preprocessed_text'].progress_apply(lambda x: sentiment_pipeline(x)[0]['score'])    \n",
    "\n",
    "finclas_df.to_csv(\"Dataset/merged_datasetA(labeled).csv\", index=False)\n",
    "print(finclas_df.head())\n",
    "print( len(finclas_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce9077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  Here are thursdays biggest analyst calls Apple...   \n",
      "1  Buy as vegas hands as travel to singapore buil...   \n",
      "2  piper handler downgrades DocuSign to sell citi...   \n",
      "3  analysts react to Teslas latest earnings break...   \n",
      "4  netflix and its peers are set for a return to ...   \n",
      "5  barclays believes earnings for these underperf...   \n",
      "6  bernstein upgrades Alibaba says shares can ral...   \n",
      "\n",
      "                                                text sentiment     score  \n",
      "0  Here are Thursday's biggest analyst calls: App...   Neutral  0.999992  \n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...   Neutral  0.999866  \n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...  Negative  0.999097  \n",
      "3  Analysts react to Tesla's latest earnings, bre...   Neutral  0.999918  \n",
      "4  Netflix and its peers are set for a ‘return to...  Positive  0.999999  \n",
      "5  Barclays believes earnings for these underperf...   Neutral  0.408876  \n",
      "6  Bernstein upgrades Alibaba, says shares can ra...  Positive  1.000000  \n",
      "\n",
      " Total number of rows:  20163\n"
     ]
    }
   ],
   "source": [
    "#Load twitter_labeled\n",
    "twteda_df = pd.read_csv(\"Dataset/merged_datasetA(labeled).csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(twteda_df.head(7))\n",
    "print(\"\\n Total number of rows: \",len(twteda_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20163, 4)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20163 entries, 0 to 20162\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   preprocessed_text  20163 non-null  object \n",
      " 1   text               20163 non-null  object \n",
      " 2   sentiment          20163 non-null  object \n",
      " 3   score              20163 non-null  float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 630.2+ KB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      " preprocessed_text    0\n",
      "text                 0\n",
      "sentiment            0\n",
      "score                0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", twteda_df.shape)\n",
    "print(\"\\nInfo:\")\n",
    "print(twteda_df.info())\n",
    "print(\"\\nMissing values:\\n\", twteda_df.isnull().sum())\n",
    "print(\"\\nDuplicate rows:\", twteda_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b90c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Length per Label Statistics:\n",
      "==================================================\n",
      "+-------------+-------+-------+--------+----------+-------+---------+\n",
      "|  sentiment  |  max  |  min  |  mean  |  median  |  std  |  count  |\n",
      "+=============+=======+=======+========+==========+=======+=========+\n",
      "|  Negative   |   1   |   1   |   1    |    1     |   0   |  3815   |\n",
      "+-------------+-------+-------+--------+----------+-------+---------+\n",
      "|   Neutral   |   1   |   1   |   1    |    1     |   0   |  12596  |\n",
      "+-------------+-------+-------+--------+----------+-------+---------+\n",
      "|  Positive   |   1   |   1   |   1    |    1     |   0   |  3752   |\n",
      "+-------------+-------+-------+--------+----------+-------+---------+\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#Statistics\n",
    "def print_stats(stats_df, title):\n",
    "    print(f\"\\n{title} Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(stats_df.to_markdown(tablefmt=\"grid\", stralign='center', numalign='center'))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "twteda_df[\"length\"] = twteda_df[\"sentiment\"].apply(lambda x: len(x.split()))\n",
    "grouped_stats = twteda_df.groupby('sentiment')[\"length\"].agg(['max', 'min', 'mean', 'median', 'std', 'count'])\n",
    "print_stats(grouped_stats, \"Text Length per Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14226ec",
   "metadata": {},
   "source": [
    "- PReprocess for All Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc4d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from urlextract import URLExtract\n",
    "from tqdm import tqdm\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54360d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Here are Thursday's biggest analyst calls: App...      0\n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
      "3  Analysts react to Tesla's latest earnings, bre...      0\n",
      "4  Netflix and its peers are set for a ‘return to...      0\n",
      "21107\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load combined dataset\n",
    "Nompep_df = pd.read_csv(\"Dataset\\\\merged_datasetA.csv\")\n",
    "print(Nompep_df.head())\n",
    "print( len(Nompep_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a29988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JohnTan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1, prefix_length=7)\n",
    "dictionary_path = \"SymSpell_Dictionary\\\\frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "correction_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21107/21107 [07:14<00:00, 48.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "      <td>here be thursday big analyst call apple amazon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "      <td>buy las vegas sand as travel to singapore buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "      <td>piper handler downgrade docusign to sell cite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "      <td>analyst react to tesla late earning break down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "      <td>netflix and its peer be set for a return to gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Here are Thursday's biggest analyst calls: App...      0   \n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0   \n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0   \n",
       "3  Analysts react to Tesla's latest earnings, bre...      0   \n",
       "4  Netflix and its peers are set for a ‘return to...      0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  here be thursday big analyst call apple amazon...  \n",
       "1  buy las vegas sand as travel to singapore buil...  \n",
       "2  piper handler downgrade docusign to sell cite ...  \n",
       "3  analyst react to tesla late earning break down...  \n",
       "4  netflix and its peer be set for a return to gr...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spell_correct(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=1)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    def is_valid_text(text):\n",
    "        return isinstance(text, str) and text.strip() != ''\n",
    "\n",
    "    def is_english(text, target_language='en'):\n",
    "        try:\n",
    "            return detect(text) == target_language\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def text_cleaning(text):\n",
    "        # Lowecase text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Detect Emoji and Replace with text\n",
    "        text = emoji.replace_emoji(text, replace=lambda e, _: emoji.demojize(e).strip(':').replace('_', ' '))\n",
    "\n",
    "        # Detect and Expend contraction\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # Detect and Remove HTML Tag\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "        # Identify and Remove URLs\n",
    "        extractor = URLExtract()\n",
    "        urls = extractor.find_urls(text)\n",
    "        for url in urls:\n",
    "            text = text.replace(url, '') \n",
    "\n",
    "        # Detect and Remove Punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "        # Detect and Remove Special Symbol\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "        # Correct Misspelled Word\n",
    "        text = spell_correct(text)\n",
    "\n",
    "        # Lemmatize The Text\n",
    "        doc = nlp(text)\n",
    "        text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "        return text\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    if not is_valid_text(text):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    if not is_english(text):\n",
    "        return \"\"\n",
    "\n",
    "    return text_cleaning(text)\n",
    "   \n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "Nompep_df['preprocessed_text'] = Nompep_df['text'].progress_apply(preprocess)\n",
    "Nompep_df = Nompep_df[Nompep_df['preprocessed_text'].str.strip() != '']\n",
    "Nompep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f274824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Metric         Before         After\n",
      "0   Avg text length (chars)     138.265262    107.217952\n",
      "1   Avg text length (words)      18.639970     17.550558\n",
      "2                Total URLs   23290.000000      0.000000\n",
      "3              Total Emojis     662.000000      0.000000\n",
      "4  Total Special Characters  195854.000000  24333.000000\n",
      "5    Total Corrections Made       0.000000      0.000000\n"
     ]
    }
   ],
   "source": [
    "# Statistic Analyze After Cleaning\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'http\\S+|www\\.\\S+', text))\n",
    "\n",
    "def count_emojis(text):\n",
    "    return len(emoji.emoji_list(text))\n",
    "\n",
    "def count_special_chars(text):\n",
    "    return len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "\n",
    "# Create metrics before and after\n",
    "insights = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Avg text length (chars)\",\n",
    "        \"Avg text length (words)\",\n",
    "        \"Total URLs\",\n",
    "        \"Total Emojis\",\n",
    "        \"Total Special Characters\",\n",
    "        \"Total Corrections Made\"\n",
    "    ],\n",
    "    \"Before\": [\n",
    "        finpep_df['text'].str.len().mean(),\n",
    "        finpep_df['text'].str.split().apply(len).mean(),\n",
    "        finpep_df['text'].apply(count_urls).sum(),\n",
    "        finpep_df['text'].apply(count_emojis).sum(),\n",
    "        finpep_df['text'].apply(count_special_chars).sum(),\n",
    "        sum(correction_counter.values())\n",
    "    ],\n",
    "    \"After\": [\n",
    "        finpep_df['preprocessed_text'].str.len().mean(),\n",
    "        finpep_df['preprocessed_text'].str.split().apply(len).mean(),\n",
    "        finpep_df['preprocessed_text'].apply(count_urls).sum(),\n",
    "        finpep_df['preprocessed_text'].apply(count_emojis).sum(),\n",
    "        finpep_df['preprocessed_text'].apply(count_special_chars).sum(),\n",
    "        sum(correction_counter.values()) \n",
    "    ]\n",
    "})\n",
    "\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2005bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  Here are Thursday's biggest analyst calls: App...      0   \n",
      "1  Buy Las Vegas Sands as travel to Singapore bui...      0   \n",
      "2  Piper Sandler downgrades DocuSign to sell, cit...      0   \n",
      "3  Analysts react to Tesla's latest earnings, bre...      0   \n",
      "4  Netflix and its peers are set for a ‘return to...      0   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  here be thursday big analyst call apple amazon...  \n",
      "1  buy las vegas sand as travel to singapore buil...  \n",
      "2  piper handler downgrade docusign to sell cite ...  \n",
      "3  analyst react to tesla late earning break down...  \n",
      "4  netflix and its peer be set for a return to gr...  \n",
      "20154\n"
     ]
    }
   ],
   "source": [
    "Nompep_df.to_csv(\"Dataset/merged_datasetA_allCase(cleaned).csv\", columns=['preprocessed_text','text'],index=False)\n",
    "print(Nompep_df.head())\n",
    "print( len(Nompep_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25c8793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20154, 3)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20154 entries, 0 to 21106\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   text               20154 non-null  object\n",
      " 1   label              20154 non-null  int64 \n",
      " 2   preprocessed_text  20154 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 629.8+ KB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      " text                 0\n",
      "label                0\n",
      "preprocessed_text    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", Nompep_df.shape)\n",
    "print(\"\\nInfo:\")\n",
    "print(Nompep_df.info())\n",
    "print(\"\\nMissing values:\\n\", Nompep_df.isnull().sum())\n",
    "print(\"\\nDuplicate rows:\", Nompep_df.duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
